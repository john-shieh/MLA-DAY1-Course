{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"./../../images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "\n",
    "# <a name=\"0\">MLU Day One Machine Learning - Code Walkthrough & Advanced AutoGluon Features</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use AutoGluon `TabularPredictor` to solve two machine learning tasks: a __regression task__ (book price prediction) and a __multiclass classification task__ (occupation prediction). \n",
    "\n",
    "<a href=\"#01\">Part I - Solution Walkthrough & Discussions</a>, covers a basic solution for the Book Price regression problem from the *MLU-DAY-ONE-ML-Hands-On.ipynb* notebook.\n",
    "\n",
    "<a href=\"#02\">Part II - Advanced AutoGluon Features</a>, dives deeper into more advanced AutoGluon features, solving a multiclass classification task of predicting the occupation of individuals using US census data.\n",
    "\n",
    "1. <a href=\"#1\">ML Problem Description</a>\n",
    "2. <a href=\"#2\">Loading the Data</a>\n",
    "3. <a href=\"#5\">Model Training with AutoGluon</a>\n",
    "    * Specifying performance metric\n",
    "    * Specifying settings for TabularPredictor\n",
    "    * Specifying hyperparameters and tuning them\n",
    "    \n",
    "4. <a href=\"#7\">Model ensembling with stacking/bagging</a>\n",
    "5. <a href=\"#8\">Prediction options (inference)</a>\n",
    "6. <a href=\"#10\">Selecting individual models for predictions</a>\n",
    "7. <a href=\"#11\">Interpretability: Feature importance</a>\n",
    "8. <a href=\"#12\">Inference Speed: Model distillation</a>\n",
    "    * Training student models\n",
    "    * Excluding models\n",
    "    \n",
    "9. <a href=\"#13\">Before You Go (clean up model artifacts)</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Jupiter notebooks environment__:\n",
    "\n",
    "* Jupiter notebooks allow creating and sharing documents that contain both code and rich text cells. If you are not familiar with Jupiter notebooks, read more [here](https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/what_is_jupyter.html). \n",
    "* This is a quick-start demo to bring you up to speed on coding and experimenting with machine learning. Move through the notebook __from top to bottom__. \n",
    "* Run each code cell to see its output. To run a cell, click within the cell and press __Shift+Enter__, or click __Run__ from the top of the page menu. \n",
    "* A `[*]` symbol next to the cell indicates the code is still running. A `[#]` symbol, where # is an integer, indicates it is finished.\n",
    "* Beware, __some code cells might take longer to run__, sometimes 5-10 minutes (depending on the task, installing packages and libraries, training models, etc.)\n",
    "\n",
    "Let's start by loading some libraries and packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q autogluon==0.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in libraries\n",
    "import pandas as pd\n",
    "# Importing the libraries needed to work with our Tabular dataset.\n",
    "from autogluon.tabular import TabularPredictor, TabularDataset\n",
    "# Additional library for tuning\n",
    "import autogluon.core as ag\n",
    "from autogluon.common import space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <a name=\"01\">Part I - Walkthrough & Discussions</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Now that you have finished your hands-on activity, let's walk through the code you have used and discuss it. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230706_190152/\"\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230706_190152/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.10\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Mon Apr 24 23:34:06 UTC 2023\n",
      "Disk Space Avail:   481.17 GB / 528.24 GB (91.1%)\n",
      "Train Data Rows:    5051\n",
      "Train Data Columns: 9\n",
      "Label Column: Price\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (4.149249912590282, 1.414973347970818, 2.60147, 0.33003)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    28874.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 10.91 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Title', 'Edition', 'Ratings', 'Synopsis']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 4920\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])          : 1 | ['ID']\n",
      "\t\t('object', [])       : 4 | ['Author', 'Reviews', 'Genre', 'BookCategory']\n",
      "\t\t('object', ['text']) : 4 | ['Title', 'Edition', 'Ratings', 'Synopsis']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :    4 | ['Author', 'Reviews', 'Genre', 'BookCategory']\n",
      "\t\t('category', ['text_as_category'])  :    4 | ['Title', 'Edition', 'Ratings', 'Synopsis']\n",
      "\t\t('int', [])                         :    1 | ['ID']\n",
      "\t\t('int', ['binned', 'text_special']) :   55 | ['Title.char_count', 'Title.word_count', 'Title.capital_ratio', 'Title.lower_ratio', 'Title.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 4859 | ['__nlp__.000', '__nlp__.10', '__nlp__.10 customer', '__nlp__.100', '__nlp__.11', ...]\n",
      "\t24.9s = Fit runtime\n",
      "\t9 features in original data used to generate 4923 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 49.48 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 25.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 4545, Val Rows: 506\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 34.31s of the 34.17s of remaining time.\n",
      "\t-0.1243\t = Validation score   (-mean_squared_error)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 32.24s of the 32.1s of remaining time.\n",
      "\t-0.1232\t = Validation score   (-mean_squared_error)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 30.24s of the 30.09s of remaining time.\n",
      "\t-0.0457\t = Validation score   (-mean_squared_error)\n",
      "\t6.99s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 23.0s of the 22.86s of remaining time.\n",
      "\t-0.0457\t = Validation score   (-mean_squared_error)\n",
      "\t6.52s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 16.25s of the 16.07s of remaining time.\n",
      "\t-0.0557\t = Validation score   (-mean_squared_error)\n",
      "\t206.01s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 34.31s of the -191.49s of remaining time.\n",
      "\t-0.0444\t = Validation score   (-mean_squared_error)\n",
      "\t0.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 251.89s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230706_190152/\")\n"
     ]
    }
   ],
   "source": [
    "# Loading the train and test datasets\n",
    "df_train = TabularDataset(\"../../data/training.csv\")\n",
    "df_test = TabularDataset(\"../../data/mlu-leaderboard-test.csv\")\n",
    "\n",
    "# Train a model with AutoGluon on the train dataset\n",
    "# Set the training time to a minute here (60 seconds), for fast experimentation\n",
    "predictor = TabularPredictor(label=\"Price\", eval_metric=\"mean_squared_error\").fit(\n",
    "    train_data=df_train, time_limit=60\n",
    ")\n",
    "\n",
    "# Make predictions on the test dataset with the AutoGluon model\n",
    "predictions = predictor.predict(df_test)\n",
    "\n",
    "# Creating a new dataframe for the MLU Leaderboard submission\n",
    "submission = df_test[[\"ID\"]].copy(deep=True)\n",
    "\n",
    "# Creating label column from price prediction list\n",
    "submission[\"Price\"] = predictions\n",
    "\n",
    "# Saving the dataframe as a csv file for MLU Leaderboard submission\n",
    "# index=False prevents printing the row IDs as separate values\n",
    "submission.to_csv(\n",
    "    \"../../data/predictions/Solution-Demo.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# <a name=\"02\">Part II - Advanced AutoGluon Features</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "---\n",
    "## <a name=\"1\">ML Problem Description</a>\n",
    "\n",
    "Predict the occupation of individuals using census data. \n",
    "> This is a __multiclass classification__ task (15 distinct classes). <br>\n",
    "\n",
    "For the advanced feature demonstration we use a new dataset: Census data. In this particular dataset, each row corresponds to an individual person, and the columns contain various demographic characteristics collected for the census.\n",
    "\n",
    "We predict the occupation of an individual - this is a multiclass classification problem. Start by importing AutoGluon’s `TabularPredictor` and `TabularDataset`, and load the data from a S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <a name=\"2\">Loading the data</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv | Columns = 15 / 15 | Rows = 39073 -> 39073\n",
      "Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv | Columns = 15 / 15 | Rows = 9769 -> 9769\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6118</th>\n",
       "      <td>51</td>\n",
       "      <td>Private</td>\n",
       "      <td>39264</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23204</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>51662</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29590</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>326310</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18116</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>222450</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "      <td>40</td>\n",
       "      <td>El-Salvador</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33964</th>\n",
       "      <td>62</td>\n",
       "      <td>Private</td>\n",
       "      <td>109190</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age workclass  fnlwgt      education  education-num  \\\n",
       "6118    51   Private   39264   Some-college             10   \n",
       "23204   58   Private   51662           10th              6   \n",
       "29590   40   Private  326310   Some-college             10   \n",
       "18116   37   Private  222450        HS-grad              9   \n",
       "33964   62   Private  109190      Bachelors             13   \n",
       "\n",
       "            marital-status        occupation    relationship    race      sex  \\\n",
       "6118    Married-civ-spouse   Exec-managerial            Wife   White   Female   \n",
       "23204   Married-civ-spouse     Other-service            Wife   White   Female   \n",
       "29590   Married-civ-spouse      Craft-repair         Husband   White     Male   \n",
       "18116        Never-married             Sales   Not-in-family   White     Male   \n",
       "33964   Married-civ-spouse   Exec-managerial         Husband   White     Male   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week  native-country   class  \n",
       "6118              0             0              40   United-States    >50K  \n",
       "23204             0             0               8   United-States   <=50K  \n",
       "29590             0             0              44   United-States   <=50K  \n",
       "18116             0          2339              40     El-Salvador   <=50K  \n",
       "33964         15024             0              40   United-States    >50K  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the dataset\n",
    "train_data = TabularDataset(\"https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv\")\n",
    "\n",
    "# Let's load the test data\n",
    "test_data = TabularDataset(\"https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv\")\n",
    "\n",
    "# Subsample a subset of data for faster demo, try setting this to much larger values\n",
    "subsample_size = 1000\n",
    "\n",
    "train_data = train_data.sample(n=subsample_size, random_state=0)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <a name=\"5\">Model Training with AutoGluon</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify eval-metric just for demo (unnecessary as it's the default)\n",
    "metric = \"accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full list of AutoGluon classification metrics can be found here:\n",
    "\n",
    "`'accuracy', 'balanced_accuracy', 'f1', 'f1_macro', 'f1_micro', 'f1_weighted', 'roc_auc', 'average_precision', 'precision', 'precision_macro', 'precision_micro', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_weighted', 'log_loss', 'pac_score'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying settings for TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train various models for ~2 min\n",
    "time_limit = 2 * 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying hyperparameters and tuning them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_options = {  # specifies non-default hyperparameter values for neural network models\n",
    "    'num_epochs': 10,  # number of training epochs (controls training time of NN models)\n",
    "    'learning_rate': space.Real(1e-4, 1e-2, default=5e-4, log=True),  # learning rate used in training (real-valued hyperparameter searched on log-scale)\n",
    "    'activation': space.Categorical('relu', 'softrelu', 'tanh'),  # activation function used in NN (categorical hyperparameter, default = first entry)\n",
    "    'dropout_prob': space.Real(0.0, 0.5, default=0.1),  # dropout probability (real-valued hyperparameter)\n",
    "}\n",
    "\n",
    "gbm_options = {  # specifies non-default hyperparameter values for lightGBM gradient boosted trees\n",
    "    'num_boost_round': 100,  # number of boosting rounds (controls training time of GBM models)\n",
    "    'num_leaves': space.Int(lower=26, upper=66, default=36),  # number of leaves in trees (integer hyperparameter)\n",
    "}\n",
    "\n",
    "hyperparameters = {  # hyperparameters of each model type\n",
    "                   'GBM': gbm_options,\n",
    "                   'NN_TORCH': nn_options,  # NOTE: comment this line out if you get errors on Mac OSX\n",
    "                  }  # When these keys are missing from hyperparameters dict, no models of that type are trained\n",
    "\n",
    "num_trials = 5  # try at most 5 different hyperparameter configurations for each type of model\n",
    "search_strategy = 'auto'  # to tune hyperparameters using random search routine with a local scheduler\n",
    "\n",
    "hyperparameter_tune_kwargs = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "    'num_trials': num_trials,\n",
    "    'scheduler' : 'local',\n",
    "    'searcher': search_strategy,\n",
    "}  # Refer to TabularPredictor.fit docstring for all valid values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitted model: NeuralNetTorch/60c80ed2 ...\n",
      "\t0.355\t = Validation score   (accuracy)\n",
      "\t2.15s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitted model: NeuralNetTorch/98025bd5 ...\n",
      "\t0.32\t = Validation score   (accuracy)\n",
      "\t2.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitted model: NeuralNetTorch/30cfc9ab ...\n",
      "\t0.325\t = Validation score   (accuracy)\n",
      "\t1.92s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitted model: NeuralNetTorch/40c14364 ...\n",
      "\t0.32\t = Validation score   (accuracy)\n",
      "\t2.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitted model: NeuralNetTorch/98098956 ...\n",
      "\t0.325\t = Validation score   (accuracy)\n",
      "\t3.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 119.9s of the 105.1s of remaining time.\n",
      "\t0.415\t = Validation score   (accuracy)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 15.55s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230706_190607/\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=\"occupation\", eval_metric=metric).fit(\n",
    "    train_data,\n",
    "    time_limit=time_limit,\n",
    "    hyperparameters=hyperparameters,\n",
    "    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following to view a summary of what happened during the fit. Now this command will show details of the hyperparameter-tuning process for each type of model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                      model  score_val  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       WeightedEnsemble_L2      0.415       0.058797  8.948627                0.000751           0.622927            2       True         11\n",
      "1               LightGBM/T3      0.375       0.004982  0.344967                0.004982           0.344967            1       True          3\n",
      "2               LightGBM/T5      0.375       0.006418  0.530014                0.006418           0.530014            1       True          5\n",
      "3               LightGBM/T1      0.370       0.004408  0.625694                0.004408           0.625694            1       True          1\n",
      "4               LightGBM/T4      0.360       0.008925  0.589324                0.008925           0.589324            1       True          4\n",
      "5               LightGBM/T2      0.355       0.005702  0.632605                0.005702           0.632605            1       True          2\n",
      "6   NeuralNetTorch/60c80ed2      0.355       0.020263  2.153493                0.020263           2.153493            1       True          6\n",
      "7   NeuralNetTorch/98098956      0.325       0.014345  3.003770                0.014345           3.003770            1       True         10\n",
      "8   NeuralNetTorch/30cfc9ab      0.325       0.015443  1.915377                0.015443           1.915377            1       True          8\n",
      "9   NeuralNetTorch/40c14364      0.320       0.009147  2.398421                0.009147           2.398421            1       True          9\n",
      "10  NeuralNetTorch/98025bd5      0.320       0.013266  2.599325                0.013266           2.599325            1       True          7\n",
      "Number of models trained: 11\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'LGBModel', 'TabularNeuralNetTorchModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])  : 6 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "('int', ['bool']) : 2 | ['sex', 'class']\n",
      "Plot summary of models saved to file: AutogluonModels/ag-20230706_190607/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'LightGBM/T1': 'LGBModel',\n",
       "  'LightGBM/T2': 'LGBModel',\n",
       "  'LightGBM/T3': 'LGBModel',\n",
       "  'LightGBM/T4': 'LGBModel',\n",
       "  'LightGBM/T5': 'LGBModel',\n",
       "  'NeuralNetTorch/60c80ed2': 'TabularNeuralNetTorchModel',\n",
       "  'NeuralNetTorch/98025bd5': 'TabularNeuralNetTorchModel',\n",
       "  'NeuralNetTorch/30cfc9ab': 'TabularNeuralNetTorchModel',\n",
       "  'NeuralNetTorch/40c14364': 'TabularNeuralNetTorchModel',\n",
       "  'NeuralNetTorch/98098956': 'TabularNeuralNetTorchModel',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'LightGBM/T1': 0.37,\n",
       "  'LightGBM/T2': 0.355,\n",
       "  'LightGBM/T3': 0.375,\n",
       "  'LightGBM/T4': 0.36,\n",
       "  'LightGBM/T5': 0.375,\n",
       "  'NeuralNetTorch/60c80ed2': 0.355,\n",
       "  'NeuralNetTorch/98025bd5': 0.32,\n",
       "  'NeuralNetTorch/30cfc9ab': 0.325,\n",
       "  'NeuralNetTorch/40c14364': 0.32,\n",
       "  'NeuralNetTorch/98098956': 0.325,\n",
       "  'WeightedEnsemble_L2': 0.415},\n",
       " 'model_best': 'WeightedEnsemble_L2',\n",
       " 'model_paths': {'LightGBM/T1': 'AutogluonModels/ag-20230706_190607/models/LightGBM/T1/',\n",
       "  'LightGBM/T2': 'AutogluonModels/ag-20230706_190607/models/LightGBM/T2/',\n",
       "  'LightGBM/T3': 'AutogluonModels/ag-20230706_190607/models/LightGBM/T3/',\n",
       "  'LightGBM/T4': 'AutogluonModels/ag-20230706_190607/models/LightGBM/T4/',\n",
       "  'LightGBM/T5': 'AutogluonModels/ag-20230706_190607/models/LightGBM/T5/',\n",
       "  'NeuralNetTorch/60c80ed2': 'AutogluonModels/ag-20230706_190607/models/NeuralNetTorch/60c80ed2/',\n",
       "  'NeuralNetTorch/98025bd5': 'AutogluonModels/ag-20230706_190607/models/NeuralNetTorch/98025bd5/',\n",
       "  'NeuralNetTorch/30cfc9ab': 'AutogluonModels/ag-20230706_190607/models/NeuralNetTorch/30cfc9ab/',\n",
       "  'NeuralNetTorch/40c14364': 'AutogluonModels/ag-20230706_190607/models/NeuralNetTorch/40c14364/',\n",
       "  'NeuralNetTorch/98098956': 'AutogluonModels/ag-20230706_190607/models/NeuralNetTorch/98098956/',\n",
       "  'WeightedEnsemble_L2': 'AutogluonModels/ag-20230706_190607/models/WeightedEnsemble_L2/'},\n",
       " 'model_fit_times': {'LightGBM/T1': 0.6256937980651855,\n",
       "  'LightGBM/T2': 0.6326050758361816,\n",
       "  'LightGBM/T3': 0.3449673652648926,\n",
       "  'LightGBM/T4': 0.5893242359161377,\n",
       "  'LightGBM/T5': 0.5300142765045166,\n",
       "  'NeuralNetTorch/60c80ed2': 2.1534934043884277,\n",
       "  'NeuralNetTorch/98025bd5': 2.599325180053711,\n",
       "  'NeuralNetTorch/30cfc9ab': 1.9153773784637451,\n",
       "  'NeuralNetTorch/40c14364': 2.398420810699463,\n",
       "  'NeuralNetTorch/98098956': 3.003769874572754,\n",
       "  'WeightedEnsemble_L2': 0.6229267120361328},\n",
       " 'model_pred_times': {'LightGBM/T1': 0.004407644271850586,\n",
       "  'LightGBM/T2': 0.00570225715637207,\n",
       "  'LightGBM/T3': 0.004981517791748047,\n",
       "  'LightGBM/T4': 0.008924722671508789,\n",
       "  'LightGBM/T5': 0.0064182281494140625,\n",
       "  'NeuralNetTorch/60c80ed2': 0.020262956619262695,\n",
       "  'NeuralNetTorch/98025bd5': 0.013266324996948242,\n",
       "  'NeuralNetTorch/30cfc9ab': 0.015443086624145508,\n",
       "  'NeuralNetTorch/40c14364': 0.009147405624389648,\n",
       "  'NeuralNetTorch/98098956': 0.014345169067382812,\n",
       "  'WeightedEnsemble_L2': 0.0007510185241699219},\n",
       " 'num_bag_folds': 0,\n",
       " 'max_stack_level': 2,\n",
       " 'num_classes': 13,\n",
       " 'model_hyperparams': {'LightGBM/T1': {'learning_rate': 0.05,\n",
       "   'num_boost_round': 100,\n",
       "   'num_leaves': 36,\n",
       "   'feature_fraction': 1.0,\n",
       "   'min_data_in_leaf': 20},\n",
       "  'LightGBM/T2': {'learning_rate': 0.06994332504138304,\n",
       "   'num_boost_round': 100,\n",
       "   'num_leaves': 29,\n",
       "   'feature_fraction': 0.8872033759818312,\n",
       "   'min_data_in_leaf': 5},\n",
       "  'LightGBM/T3': {'learning_rate': 0.04988344687833528,\n",
       "   'num_boost_round': 100,\n",
       "   'num_leaves': 62,\n",
       "   'feature_fraction': 0.9618129346960314,\n",
       "   'min_data_in_leaf': 52},\n",
       "  'LightGBM/T4': {'learning_rate': 0.006163502781172814,\n",
       "   'num_boost_round': 100,\n",
       "   'num_leaves': 27,\n",
       "   'feature_fraction': 0.824383651636118,\n",
       "   'min_data_in_leaf': 14},\n",
       "  'LightGBM/T5': {'learning_rate': 0.035179640321040824,\n",
       "   'num_boost_round': 100,\n",
       "   'num_leaves': 43,\n",
       "   'feature_fraction': 0.9479312595206661,\n",
       "   'min_data_in_leaf': 26},\n",
       "  'NeuralNetTorch/60c80ed2': {'num_epochs': 10,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'activation': 'relu',\n",
       "   'embedding_size_factor': 1.0,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'dropout_prob': 0.1,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.0005,\n",
       "   'weight_decay': 1e-06,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'median',\n",
       "   'proc.max_category_levels': 100,\n",
       "   'proc.skew_threshold': 0.99,\n",
       "   'use_ngram_features': False,\n",
       "   'num_layers': 2,\n",
       "   'hidden_size': 128,\n",
       "   'max_batch_size': 512,\n",
       "   'use_batchnorm': False,\n",
       "   'loss_function': 'auto'},\n",
       "  'NeuralNetTorch/98025bd5': {'num_epochs': 10,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'activation': 'relu',\n",
       "   'embedding_size_factor': 1.5,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'dropout_prob': 0.010014895545472746,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.0028080013210490984,\n",
       "   'weight_decay': 0.0023892655019361235,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'median',\n",
       "   'proc.max_category_levels': 20,\n",
       "   'proc.skew_threshold': 0.99,\n",
       "   'use_ngram_features': False,\n",
       "   'num_layers': 4,\n",
       "   'hidden_size': 256,\n",
       "   'max_batch_size': 512,\n",
       "   'use_batchnorm': True,\n",
       "   'loss_function': 'auto'},\n",
       "  'NeuralNetTorch/30cfc9ab': {'num_epochs': 10,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'activation': 'softrelu',\n",
       "   'embedding_size_factor': 1.2,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'dropout_prob': 0.01460340557151213,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.000462689627762297,\n",
       "   'weight_decay': 1.435167636251011e-09,\n",
       "   'proc.embed_min_categories': 10,\n",
       "   'proc.impute_strategy': 'mean',\n",
       "   'proc.max_category_levels': 300,\n",
       "   'proc.skew_threshold': 0.8,\n",
       "   'use_ngram_features': False,\n",
       "   'num_layers': 3,\n",
       "   'hidden_size': 128,\n",
       "   'max_batch_size': 512,\n",
       "   'use_batchnorm': False,\n",
       "   'loss_function': 'auto'},\n",
       "  'NeuralNetTorch/40c14364': {'num_epochs': 10,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'activation': 'tanh',\n",
       "   'embedding_size_factor': 1.3,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'dropout_prob': 0.4180005049676207,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.0009793507122125302,\n",
       "   'weight_decay': 8.616976509388855e-05,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'mean',\n",
       "   'proc.max_category_levels': 10,\n",
       "   'proc.skew_threshold': 100.0,\n",
       "   'use_ngram_features': False,\n",
       "   'num_layers': 3,\n",
       "   'hidden_size': 256,\n",
       "   'max_batch_size': 512,\n",
       "   'use_batchnorm': True,\n",
       "   'loss_function': 'auto'},\n",
       "  'NeuralNetTorch/98098956': {'num_epochs': 10,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'activation': 'relu',\n",
       "   'embedding_size_factor': 0.6,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'dropout_prob': 0.1415433049113322,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.007179666323723652,\n",
       "   'weight_decay': 9.117606525053729e-09,\n",
       "   'proc.embed_min_categories': 1000,\n",
       "   'proc.impute_strategy': 'mean',\n",
       "   'proc.max_category_levels': 200,\n",
       "   'proc.skew_threshold': 0.99,\n",
       "   'use_ngram_features': False,\n",
       "   'num_layers': 3,\n",
       "   'hidden_size': 512,\n",
       "   'max_batch_size': 512,\n",
       "   'use_batchnorm': True,\n",
       "   'loss_function': 'auto'},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                       model  score_val  pred_time_val  fit_time  \\\n",
       " 0       WeightedEnsemble_L2      0.415       0.058797  8.948627   \n",
       " 1               LightGBM/T3      0.375       0.004982  0.344967   \n",
       " 2               LightGBM/T5      0.375       0.006418  0.530014   \n",
       " 3               LightGBM/T1      0.370       0.004408  0.625694   \n",
       " 4               LightGBM/T4      0.360       0.008925  0.589324   \n",
       " 5               LightGBM/T2      0.355       0.005702  0.632605   \n",
       " 6   NeuralNetTorch/60c80ed2      0.355       0.020263  2.153493   \n",
       " 7   NeuralNetTorch/98098956      0.325       0.014345  3.003770   \n",
       " 8   NeuralNetTorch/30cfc9ab      0.325       0.015443  1.915377   \n",
       " 9   NeuralNetTorch/40c14364      0.320       0.009147  2.398421   \n",
       " 10  NeuralNetTorch/98025bd5      0.320       0.013266  2.599325   \n",
       " \n",
       "     pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                 0.000751           0.622927            2       True   \n",
       " 1                 0.004982           0.344967            1       True   \n",
       " 2                 0.006418           0.530014            1       True   \n",
       " 3                 0.004408           0.625694            1       True   \n",
       " 4                 0.008925           0.589324            1       True   \n",
       " 5                 0.005702           0.632605            1       True   \n",
       " 6                 0.020263           2.153493            1       True   \n",
       " 7                 0.014345           3.003770            1       True   \n",
       " 8                 0.015443           1.915377            1       True   \n",
       " 9                 0.009147           2.398421            1       True   \n",
       " 10                0.013266           2.599325            1       True   \n",
       " \n",
       "     fit_order  \n",
       " 0          11  \n",
       " 1           3  \n",
       " 2           5  \n",
       " 3           1  \n",
       " 4           4  \n",
       " 5           2  \n",
       " 6           6  \n",
       " 7          10  \n",
       " 8           8  \n",
       " 9           9  \n",
       " 10          7  }"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the predictive performance may be poor because we are using few training data points and small ranges for hyperparameters to ensure quick run times. You can call `fit()` multiple times while modifying these settings to better understand how these choices affect performance outcomes. For example: you can increase `subsample_size` to train using a larger dataset, increase the `num_epochs` and `num_boost_round` hyperparameters, and increase the `time_limit` (which you should do for all code in these tutorials). To see more detailed output during the execution of `fit()`, you can also pass in the argument: `verbosity = 3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <a name=\"7\">Model ensembling with stacking/bagging</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Beyond hyperparameter-tuning with a correctly-specified evaluation metric, there are two other methods to boost predictive performance:\n",
    "- bagging and \n",
    "- stack-ensembling\n",
    "\n",
    "You’ll often see performance improve if you specify `num_bag_folds = 5-10`, `num_stack_levels = 1-3` in the call to `fit()`. Beware that doing this will increase training times and memory/disk usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230706_190623/\"\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230706_190623/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.10\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Mon Apr 24 23:34:06 UTC 2023\n",
      "Disk Space Avail:   480.81 GB / 528.24 GB (91.0%)\n",
      "Train Data Rows:    1000\n",
      "Train Data Columns: 14\n",
      "Label Column: occupation\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 15) unique label values:  [' Exec-managerial', ' Other-service', ' Craft-repair', ' Sales', ' Prof-specialty', ' Protective-serv', ' ?', ' Adm-clerical', ' Machine-op-inspct', ' Tech-support']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 13 out of 15 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.996\n",
      "Train Data Class Count: 13\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    29974.18 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.58 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 6 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('int', ['bool']) : 2 | ['sex', 'class']\n",
      "\t0.1s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\t0.1155\t = Validation score   (accuracy)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\t0.0994\t = Validation score   (accuracy)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3394\t = Validation score   (accuracy)\n",
      "\t4.64s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3805\t = Validation score   (accuracy)\n",
      "\t2.37s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3635\t = Validation score   (accuracy)\n",
      "\t2.55s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ...\n",
      "\t0.3173\t = Validation score   (accuracy)\n",
      "\t0.92s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ...\n",
      "\t0.3032\t = Validation score   (accuracy)\n",
      "\t0.91s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3715\t = Validation score   (accuracy)\n",
      "\t134.82s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ...\n",
      "\t0.3052\t = Validation score   (accuracy)\n",
      "\t0.89s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ...\n",
      "\t0.3022\t = Validation score   (accuracy)\n",
      "\t0.88s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3645\t = Validation score   (accuracy)\n",
      "\t2.83s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3795\t = Validation score   (accuracy)\n",
      "\t8.88s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3042\t = Validation score   (accuracy)\n",
      "\t4.8s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t0.3805\t = Validation score   (accuracy)\n",
      "\t0.99s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 11 L2 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3514\t = Validation score   (accuracy)\n",
      "\t3.89s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3614\t = Validation score   (accuracy)\n",
      "\t22.04s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3695\t = Validation score   (accuracy)\n",
      "\t37.74s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ...\n",
      "\t0.3484\t = Validation score   (accuracy)\n",
      "\t1.07s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ...\n",
      "\t0.3514\t = Validation score   (accuracy)\n",
      "\t1.99s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3795\t = Validation score   (accuracy)\n",
      "\t245.93s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ...\n",
      "\t0.3604\t = Validation score   (accuracy)\n",
      "\t0.95s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ...\n",
      "\t0.3373\t = Validation score   (accuracy)\n",
      "\t0.94s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3464\t = Validation score   (accuracy)\n",
      "\t34.68s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3655\t = Validation score   (accuracy)\n",
      "\t5.0s\t = Training   runtime\n",
      "\t0.6s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3504\t = Validation score   (accuracy)\n",
      "\t126.47s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t0.3795\t = Validation score   (accuracy)\n",
      "\t0.83s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 679.47s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230706_190623/\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=\"occupation\", eval_metric=metric).fit(\n",
    "    train_data,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=1,\n",
    "    num_stack_levels=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should not provide `tuning_data` when stacking/bagging, and instead provide all your available data as train_data (which AutoGluon will split in more intelligent ways). Parameter `num_bag_sets` controls how many times the K-fold bagging process is repeated to further reduce variance (increasing this may further boost accuracy but will substantially increase training times, inference latency, and memory/disk usage). Rather than manually searching for good bagging/stacking values yourself, AutoGluon will automatically select good values for you if you specify `auto_stack` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"agModels-predictOccupation/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.10\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Mon Apr 24 23:34:06 UTC 2023\n",
      "Disk Space Avail:   480.21 GB / 528.24 GB (90.9%)\n",
      "Train Data Rows:    1000\n",
      "Train Data Columns: 14\n",
      "Label Column: occupation\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 15) unique label values:  [' Exec-managerial', ' Other-service', ' Craft-repair', ' Sales', ' Prof-specialty', ' Protective-serv', ' ?', ' Adm-clerical', ' Machine-op-inspct', ' Tech-support']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 13 out of 15 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.996\n",
      "Train Data Class Count: 13\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    29885.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.58 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 6 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('int', ['bool']) : 2 | ['sex', 'class']\n",
      "\t0.1s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 19.92s of the 29.89s of remaining time.\n",
      "\t0.1155\t = Validation score   (accuracy)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 19.9s of the 29.87s of remaining time.\n",
      "\t0.0994\t = Validation score   (accuracy)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 19.88s of the 29.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3544\t = Validation score   (accuracy)\n",
      "\t4.7s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 12.55s of the 22.52s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3876\t = Validation score   (accuracy)\n",
      "\t2.49s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 6.8s of the 16.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3685\t = Validation score   (accuracy)\n",
      "\t3.95s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.89s of the 9.36s of remaining time.\n",
      "\t0.3876\t = Validation score   (accuracy)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 11 L2 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 8.93s of the 8.91s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3614\t = Validation score   (accuracy)\n",
      "\t4.66s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1.53s of the 1.51s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t0.3725\t = Validation score   (accuracy)\n",
      "\t1.8s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 29.89s of the -3.83s of remaining time.\n",
      "\t0.3725\t = Validation score   (accuracy)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 34.06s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictOccupation/\")\n"
     ]
    }
   ],
   "source": [
    "# Folder where to store trained models\n",
    "save_path = \"agModels-predictOccupation\"\n",
    "\n",
    "predictor = TabularPredictor(label=\"occupation\", eval_metric=metric, path=save_path).fit(\n",
    "    train_data,\n",
    "    auto_stack=True,\n",
    "    time_limit=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often stacking/bagging will produce superior accuracy than hyperparameter-tuning, but you may try combining both techniques (note: specifying `presets='best_quality'` in `fit()` simply sets `auto_stack=True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <a name=\"8\">Prediction options (inference)</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Even if you’ve started a new Python session since last calling `fit()`, you can still load a previously trained predictor from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `predictor.path` is another way to get the relative path needed to later load predictor.\n",
    "predictor = TabularPredictor.load(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above `save_path` is the same folder previously passed to `TabularPredictor`, in which all the trained models have been saved. You can train easily models on one machine and deploy them on another. Simply copy the `save_path` folder to the new machine and specify its new path in `TabularPredictor.load()`.\n",
    "\n",
    "We can make a prediction on an individual example rather than on a full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Other-service\n",
       "Name: occupation, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select one datapoint to make a prediction\n",
    "datapoint = test_data.iloc[[0]] # Note: .iloc[0] won't work because it returns pandas Series instead of DataFrame\n",
    "\n",
    "predictor.predict(datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To output predicted class probabilities instead of predicted classes, you can use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>?</th>\n",
       "      <th>Adm-clerical</th>\n",
       "      <th>Armed-Forces</th>\n",
       "      <th>Craft-repair</th>\n",
       "      <th>Exec-managerial</th>\n",
       "      <th>Farming-fishing</th>\n",
       "      <th>Handlers-cleaners</th>\n",
       "      <th>Machine-op-inspct</th>\n",
       "      <th>Other-service</th>\n",
       "      <th>Priv-house-serv</th>\n",
       "      <th>Prof-specialty</th>\n",
       "      <th>Protective-serv</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Tech-support</th>\n",
       "      <th>Transport-moving</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038548</td>\n",
       "      <td>0.234544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050518</td>\n",
       "      <td>0.06077</td>\n",
       "      <td>0.008894</td>\n",
       "      <td>0.073112</td>\n",
       "      <td>0.048278</td>\n",
       "      <td>0.298019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.049102</td>\n",
       "      <td>0.004876</td>\n",
       "      <td>0.085449</td>\n",
       "      <td>0.021308</td>\n",
       "      <td>0.026581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ?   Adm-clerical   Armed-Forces   Craft-repair   Exec-managerial  \\\n",
       "0  0.038548       0.234544            0.0       0.050518           0.06077   \n",
       "\n",
       "    Farming-fishing   Handlers-cleaners   Machine-op-inspct   Other-service  \\\n",
       "0          0.008894            0.073112            0.048278        0.298019   \n",
       "\n",
       "    Priv-house-serv   Prof-specialty   Protective-serv     Sales  \\\n",
       "0               0.0         0.049102          0.004876  0.085449   \n",
       "\n",
       "    Tech-support   Transport-moving  \n",
       "0       0.021308           0.026581  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a DataFrame that shows which probability corresponds to which class\n",
    "predictor.predict_proba(datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `predict()` and `predict_proba()` will utilize the model that AutoGluon thinks is most accurate, which is usually an ensemble of many individual models. Here’s how to see which model this corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WeightedEnsemble_L2'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.get_model_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <a name=\"10\">Selecting individual models for predictions</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We can specify a particular model to use for predictions (e.g. to reduce inference latency). Note that a ‘model’ in AutoGluon may refer to for example a single Neural Network, a bagged ensemble of many Neural Network copies trained on different training/validation splits, a weighted ensemble that aggregates the predictions of many other models, or a stacked model that operates on predictions output by other models. This is akin to viewing a RandomForest as one ‘model’ when it is in fact an ensemble of many decision trees.\n",
    "\n",
    "\n",
    "Here’s how to specify a particular model to use for prediction instead of AutoGluon’s default model-choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from KNeighborsUnif_BAG_L1 model:  Adm-clerical\n"
     ]
    }
   ],
   "source": [
    "# index of model to use\n",
    "i = 0\n",
    "model_to_use = predictor.get_model_names()[i]\n",
    "model_pred = predictor.predict(datapoint, model=model_to_use)\n",
    "print(f\"Prediction from {model_to_use} model: {model_pred.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily access information about the trained predictor or a particular model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = predictor.get_model_names()\n",
    "model_to_use = all_models[i]\n",
    "specific_model = predictor._trainer.load_model(model_to_use)\n",
    "\n",
    "# Objects defined below are dicts with information (not printed here as they are quite large):\n",
    "model_info = specific_model.get_info()\n",
    "predictor_information = predictor.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the label columns remains in the `test_data` DataFrame, we can instead use the shorthand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: accuracy on test data: 0.35438632408639575\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"accuracy\": 0.35438632408639575,\n",
      "    \"balanced_accuracy\": 0.2380254766456843,\n",
      "    \"mcc\": 0.27835195719142447\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.35438632408639575,\n",
       " 'balanced_accuracy': 0.2380254766456843,\n",
       " 'mcc': 0.27835195719142447}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <a name=\"11\">Interpretability: Feature importance</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "To better understand our trained predictor, we can estimate the overall importance of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 14 features using 5000 rows with 5 shuffle sets...\n",
      "\t47.66s\t= Expected runtime (9.53s per shuffle set)\n",
      "\t40.26s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>workclass</th>\n",
       "      <td>0.07032</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>1.445937e-07</td>\n",
       "      <td>5</td>\n",
       "      <td>0.075119</td>\n",
       "      <td>0.065521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>0.06248</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>5.435720e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.073058</td>\n",
       "      <td>0.051902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education-num</th>\n",
       "      <td>0.05004</td>\n",
       "      <td>0.006349</td>\n",
       "      <td>3.043899e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.063112</td>\n",
       "      <td>0.036968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours-per-week</th>\n",
       "      <td>0.02208</td>\n",
       "      <td>0.006774</td>\n",
       "      <td>9.419456e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.036029</td>\n",
       "      <td>0.008131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <td>0.02004</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>6.846919e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.026469</td>\n",
       "      <td>0.013611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>0.01792</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>1.016721e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.024277</td>\n",
       "      <td>0.011563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.00752</td>\n",
       "      <td>0.003466</td>\n",
       "      <td>4.164539e-03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014656</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relationship</th>\n",
       "      <td>0.00144</td>\n",
       "      <td>0.002609</td>\n",
       "      <td>1.423639e-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006812</td>\n",
       "      <td>-0.003932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fnlwgt</th>\n",
       "      <td>0.00116</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>8.147191e-02</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004288</td>\n",
       "      <td>-0.001968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <td>0.00048</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>2.372046e-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003282</td>\n",
       "      <td>-0.002322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>native-country</th>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>2.997389e-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>-0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital-status</th>\n",
       "      <td>-0.00044</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>8.727166e-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>-0.001964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital-gain</th>\n",
       "      <td>-0.00084</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>9.056980e-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>-0.003283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital-loss</th>\n",
       "      <td>-0.00104</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>9.907187e-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>-0.002289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                importance    stddev       p_value  n  p99_high   p99_low\n",
       "workclass          0.07032  0.002331  1.445937e-07  5  0.075119  0.065521\n",
       "sex                0.06248  0.005137  5.435720e-06  5  0.073058  0.051902\n",
       "education-num      0.05004  0.006349  3.043899e-05  5  0.063112  0.036968\n",
       "hours-per-week     0.02208  0.006774  9.419456e-04  5  0.036029  0.008131\n",
       "class              0.02004  0.003122  6.846919e-05  5  0.026469  0.013611\n",
       "education          0.01792  0.003087  1.016721e-04  5  0.024277  0.011563\n",
       "age                0.00752  0.003466  4.164539e-03  5  0.014656  0.000384\n",
       "relationship       0.00144  0.002609  1.423639e-01  5  0.006812 -0.003932\n",
       "fnlwgt             0.00116  0.001519  8.147191e-02  5  0.004288 -0.001968\n",
       "race               0.00048  0.001361  2.372046e-01  5  0.003282 -0.002322\n",
       "native-country     0.00024  0.000942  2.997389e-01  5  0.002180 -0.001700\n",
       "marital-status    -0.00044  0.000740  8.727166e-01  5  0.001084 -0.001964\n",
       "capital-gain      -0.00084  0.001187  9.056980e-01  5  0.001603 -0.003283\n",
       "capital-loss      -0.00104  0.000607  9.907187e-01  5  0.000209 -0.002289"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.feature_importance(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computed via permutation-shuffling, these feature importance scores quantify the drop in predictive performance (of the already trained predictor) when one columns values are randomly shuffled across rows. The top features in this list contribute most to AutoGluon’s accuracy. Features with non-positive importance score hardly contribute to the predictors accuracy, or may even be actively harmful to include in the data (consider removing these features from your data and calling `fit` again). These scores facilitate interpretability of the predictors global behavior (which features it relies on for all predictions) rather than local explanations that only rationalize one particular prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <a name=\"12\"> Inference Speed: Model distillation</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "While computationally-favorable, single individual models will usually have lower accuracy than weighted/stacked/bagged ensembles. Model Distillation offers one way to retain the computational benefits of a single model, while enjoying some of the accuracy-boost that comes with ensembling. The idea is to train the individual model (which we can call the student) to mimic the predictions of the full stack ensemble (the teacher). Like `refit_full()`, the `distill()` function will produce additional models we can opt to use for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training student models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Distilling with teacher='WeightedEnsemble_L2', teacher_preds=soft, augment_method=spunge ...\n",
      "SPUNGE: Augmenting training data with 3980 synthetic samples for distillation...\n",
      "Distilling with each of these student models: ['LightGBM_DSTL', 'RandomForestMSE_DSTL', 'CatBoost_DSTL', 'NeuralNetTorch_DSTL']\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: LightGBM_DSTL ... Training model for up to 30.0s of the 30.0s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's soft_log_loss: -1.67598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tNote: model has different eval_metric than default.\n",
      "\t-1.6746\t = Validation score   (-soft_log_loss)\n",
      "\t19.47s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_DSTL ... Training model for up to 8.61s of the 8.6s of remaining time.\n",
      "\tNote: model has different eval_metric than default.\n",
      "\t-1.7786\t = Validation score   (-soft_log_loss)\n",
      "\t1.67s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost_DSTL ... Training model for up to 6.59s of the 6.59s of remaining time.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/catboost/core.py:2268: UserWarning: Can't optimze method \"evaluate\" because self argument is used\n",
      "  _check_train_params(params)\n",
      "\tRan out of time, early stopping on iteration 1.\n",
      "\tNote: model has different eval_metric than default.\n",
      "\t-2.5073\t = Validation score   (-soft_log_loss)\n",
      "\t9.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Distilling with each of these student models: ['WeightedEnsemble_L2_DSTL']\n",
      "Fitting model: WeightedEnsemble_L2_DSTL ... Training model for up to 30.0s of the -2.89s of remaining time.\n",
      "\tNote: model has different eval_metric than default.\n",
      "\t-1.6746\t = Validation score   (-soft_log_loss)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Distilled model leaderboard:\n",
      "                      model  score_val  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0             LightGBM_DSTL       0.42       0.200172  19.469086                0.200172          19.469086            1       True         10\n",
      "1  WeightedEnsemble_L2_DSTL       0.42       0.200693  19.543399                0.000521           0.074314            2       True         13\n",
      "2      RandomForestMSE_DSTL       0.39       0.066059   1.669453                0.066059           1.669453            1       True         11\n",
      "3             CatBoost_DSTL       0.38       0.005332   9.432172                0.005332           9.432172            1       True         12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['LightGBM_DSTL',\n",
       " 'RandomForestMSE_DSTL',\n",
       " 'CatBoost_DSTL',\n",
       " 'WeightedEnsemble_L2_DSTL']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify much longer time limit in real applications\n",
    "student_models = predictor.distill(time_limit=30)\n",
    "student_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions from LightGBM_DSTL: [' Adm-clerical', ' Farming-fishing', ' Exec-managerial', ' Sales', ' Other-service']\n"
     ]
    }
   ],
   "source": [
    "preds_student = predictor.predict(test_data, model=student_models[0])\n",
    "print(f\"predictions from {student_models[0]}: {list(preds_student)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding models\n",
    "\n",
    "Finally, you may also exclude specific unwieldy models from being trained at all. Below we exclude models that tend to be slower (K Nearest Neighbors, Neural Network, models with custom larger-than-default hyperparameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230706_192017/\"\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230706_192017/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.10\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Mon Apr 24 23:34:06 UTC 2023\n",
      "Disk Space Avail:   479.87 GB / 528.24 GB (90.8%)\n",
      "Train Data Rows:    1000\n",
      "Train Data Columns: 14\n",
      "Label Column: occupation\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 15) unique label values:  [' Exec-managerial', ' Other-service', ' Craft-repair', ' Sales', ' Prof-specialty', ' Protective-serv', ' ?', ' Adm-clerical', ' Machine-op-inspct', ' Tech-support']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 13 out of 15 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.996\n",
      "Train Data Class Count: 13\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    29419.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.58 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 6 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('int', ['bool']) : 2 | ['sex', 'class']\n",
      "\t0.1s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 796, Val Rows: 200\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 29.88s of the 29.88s of remaining time.\n",
      "\t0.37\t = Validation score   (accuracy)\n",
      "\t1.4s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 28.45s of the 28.45s of remaining time.\n",
      "\t0.385\t = Validation score   (accuracy)\n",
      "\t1.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 27.06s of the 27.06s of remaining time.\n",
      "\t0.37\t = Validation score   (accuracy)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 25.28s of the 25.28s of remaining time.\n",
      "\t0.355\t = Validation score   (accuracy)\n",
      "\t0.94s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 24.21s of the 24.2s of remaining time.\n",
      "\t0.355\t = Validation score   (accuracy)\n",
      "\t0.91s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 23.17s of the 23.16s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 508.\n",
      "\t0.39\t = Validation score   (accuracy)\n",
      "\t23.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini ... Training model for up to 0.02s of the 0.02s of remaining time.\n",
      "\tWarning: Model is expected to require 1.0s to train, which exceeds the maximum time limit of 0.0s, skipping model...\n",
      "\tTime limit exceeded... Skipping ExtraTreesGini.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.88s of the -0.03s of remaining time.\n",
      "\t0.415\t = Validation score   (accuracy)\n",
      "\t0.38s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 30.43s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230706_192017/\")\n"
     ]
    }
   ],
   "source": [
    "excluded_model_types = [\"KNN\", \"NN\", \"custom\"]\n",
    "predictor_light = TabularPredictor(label=\"occupation\", eval_metric=metric).fit(\n",
    "    train_data, excluded_model_types=excluded_model_types, time_limit=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <a name=\"13\">Before You Go</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "After you are done with this Demo, clean model artifacts by uncommenting and executing the cell below.\n",
    "\n",
    "__It is always good practice to clean everything when you are done, preventing the disk from getting full.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r AutogluonModels\n",
    "!rm -r agModels-predictOccupation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"./../../images/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "# Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
